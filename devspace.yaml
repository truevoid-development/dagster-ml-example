version: v2beta1
name: devspace-iceberg
pipelines:
  dev:
    run: |-
      run_dependency_pipelines --all
      ensure_pull_secrets --all
      build_images --all -t ${DEVSPACE_RANDOM}
      create_deployments --all
      start_dev --all

images:
  hive-metastore:
    image: truevoid/hive-metastore
    dockerfile: hive-metastore/Dockerfile
    context: hive-metastore

  python:
    image: truevoid/python
    dockerfile: python-hdfs/Dockerfile
    context: python-hdfs

deployments:
  postgres:
    helm:
      chart:
        name: postgresql
        repo: https://charts.bitnami.com/bitnami
      values:
        global:
          postgresql:
            auth:
              postgresPassword: hive
              username: hive
              password: hive
              database: hive
        primary:
          livenessProbe:
            enabled: false
          resourcePreset: medium
          persistence:
            enabled: false
          nodeSelector: &node_selector
            kubernetes.io/hostname: node-dedicated-0

  hive-metastore:
    helm:
      values:
        nodeSelector: *node_selector
        containers:
        - image: truevoid/hive-metastore:${DEVSPACE_RANDOM}
          name: hive-metastore
          env:
          - name: SERVICE_NAME
            value: metastore
          - name: DB_DRIVER
            value: postgres
          - name: SERVICE_OPTS
            value: >-
              -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver
              -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres-postgresql-hl:5432/hive
              -Djavax.jdo.option.ConnectionUserName=hive
              -Djavax.jdo.option.ConnectionPassword=hive
        service:
          name: hive-metastore
          ports:
          - containerPort: 9083
            port: 9083

  datanode:
    helm:
      values:
        nodeSelector: *node_selector
        containers:
        - image: docker.io/apache/hadoop:3
          name: datanode
          command: [/opt/starter.sh, hdfs, datanode]
          env:
          - name: ENSURE_NAMENODE_DIR
            value: /tmp/hadoop-hadoop/dfs/name
          - name: CORE-SITE.XML_fs.default.name
            value: hdfs://namenode
          - name: CORE-SITE.XML_fs.defaultFS
            value: hdfs://namenode
          - name: CORE-SITE.XML_dfs.permissions
            value: "false"
          - name: CORE-SITE.XML_dfs.namenode.rpc-address
            value: namenode:8020
          - name: CORE-SITE.XML_dfs.replication
            value: "1"
        service:
          name: datanode
          clusterIP: None
          ports:
          - containerPort: 9864
            port: 9864

  namenode:
    helm:
      values:
        nodeSelector: *node_selector
        containers:
        - image: docker.io/apache/hadoop:3
          name: namenode
          command: [/opt/starter.sh, hdfs, namenode]
          env:
          - name: ENSURE_NAMENODE_DIR
            value: /tmp/hadoop-hadoop/dfs/name
          - name: CORE-SITE.XML_fs.default.name
            value: hdfs://namenode
          - name: CORE-SITE.XML_fs.defaultFS
            value: hdfs://namenode
          - name: CORE-SITE.XML_dfs.permissions
            value: "false"
          - name: CORE-SITE.XML_dfs.namenode.rpc-address
            value: 0.0.0.0:8020
          - name: CORE-SITE.XML_dfs.replication
            value: "1"
        service:
          name: namenode
          clusterIP: None
          ports:
          - containerPort: 9870
            port: 9870
          - containerPort: 8020
            port: 8020

  spark:
    helm:
      chart:
        name: spark
        repo: https://charts.bitnami.com/bitnami
      values:
        master: &spark_config
          nodeSelector: *node_selector
        worker: *spark_config

  trino:
    helm:
      chart:
        name: trino
        repo: https://trinodb.github.io/charts/
      values:
        coordinator: &trino_config
          nodeSelector: *node_selector
          additionalNodeProperties:
          - HADOOP_USER_NAME=hadoop
        worker: *trino_config
        additionalCatalogs:
          iceberg: |-
            connector.name=iceberg
            iceberg.catalog.type=hive_metastore
            hive.metastore.uri=thrift://hive-metastore:9083
            fs.hadoop.enabled=true

  dev:
    helm:
      values:
        nodeSelector: *node_selector
        containers:
        - image: truevoid/python
          name: python
          command: [bash]
        service:
          name: pyspark
          clusterIP: None
          ports:
          - containerPort: 10000
            port: 10000

dev:
  python:
    container: python
    imageSelector: truevoid/python
    sync:
    - path: .
      excludePaths:
      - "*"
      - "!python-hdfs/main.py"
    terminal:
      command: bash
      disableScreen: true
