version: v2beta1
name: devspace-iceberg
pipelines:
  dev:
    run: |-
      run_dependency_pipelines --all
      ensure_pull_secrets --all
      build_images --all -t ${DEVSPACE_RANDOM}
      create_deployments --all
      start_dev --all

images:
  hive-metastore:
    image: truevoid/hive-metastore
    dockerfile: hive-metastore/Dockerfile
    context: hive-metastore

  python:
    image: truevoid/python
    dockerfile: python-hdfs/Dockerfile
    context: python-hdfs

deployments:
  postgres:
    helm:
      chart:
        name: postgresql
        repo: https://charts.bitnami.com/bitnami
      values:
        global:
          postgresql:
            auth:
              postgresPassword: hive
              username: hive
              password: hive
              database: hive
        primary:
          livenessProbe:
            enabled: false
          resourcePreset: medium
          persistence:
            enabled: false
          nodeSelector: &node_selector
            kubernetes.io/hostname: node-dedicated-0

  hive-metastore:
    helm:
      values:
        nodeSelector: *node_selector
        containers:
        - image: truevoid/hive-metastore:${DEVSPACE_RANDOM}
          name: hive-metastore
          env:
          - name: SERVICE_NAME
            value: metastore
          - name: DB_DRIVER
            value: postgres
          - name: SERVICE_OPTS
            value: >-
              -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver
              -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres-postgresql-hl:5432/hive
              -Djavax.jdo.option.ConnectionUserName=hive
              -Djavax.jdo.option.ConnectionPassword=hive
        service:
          name: hive-metastore
          ports:
          - containerPort: 9083
            port: 9083

  minio:
    helm:
      chart:
        name: minio
        repo: https://charts.bitnami.com/bitnami
      values:
        auth:
          rootPassword: miniopass
        defaultBuckets: storage
        persistence:
          enabled: true
        pdb:
          create: false

  nessie:
    helm:
      chart:
        path: charts/nessie

  datanode:
    helm:
      values:
        nodeSelector: *node_selector
        containers:
        - image: docker.io/apache/hadoop:3
          name: datanode
          command: [/opt/starter.sh, hdfs, datanode]
          env:
          - name: ENSURE_NAMENODE_DIR
            value: /tmp/hadoop-hadoop/dfs/name
          - name: CORE-SITE.XML_fs.default.name
            value: hdfs://namenode
          - name: CORE-SITE.XML_fs.defaultFS
            value: hdfs://namenode
          - name: CORE-SITE.XML_dfs.permissions
            value: "false"
          - name: CORE-SITE.XML_dfs.namenode.rpc-address
            value: namenode:8020
          - name: CORE-SITE.XML_dfs.replication
            value: "1"
        service:
          name: datanode
          clusterIP: None
          ports:
          - containerPort: 9864
            port: 9864

  namenode:
    helm:
      values:
        nodeSelector: *node_selector
        containers:
        - image: docker.io/apache/hadoop:3
          name: namenode
          command: [/opt/starter.sh, hdfs, namenode]
          env:
          - name: ENSURE_NAMENODE_DIR
            value: /tmp/hadoop-hadoop/dfs/name
          - name: CORE-SITE.XML_fs.default.name
            value: hdfs://namenode
          - name: CORE-SITE.XML_fs.defaultFS
            value: hdfs://namenode
          - name: CORE-SITE.XML_dfs.permissions
            value: "false"
          - name: CORE-SITE.XML_dfs.namenode.rpc-address
            value: 0.0.0.0:8020
          - name: CORE-SITE.XML_dfs.replication
            value: "1"
        service:
          name: namenode
          clusterIP: None
          ports:
          - containerPort: 9870
            port: 9870
          - containerPort: 8020
            port: 8020

  spark:
    helm:
      chart:
        name: spark
        repo: https://charts.bitnami.com/bitnami
      values:
        master: &spark_config
          nodeSelector: *node_selector
        worker: *spark_config

  trino:
    helm:
      chart:
        name: trino
        repo: https://trinodb.github.io/charts/
      values:
        coordinator: &trino_config
          nodeSelector: *node_selector
          additionalNodeProperties:
          - HADOOP_USER_NAME=hadoop
        worker: *trino_config
        additionalCatalogs:
          iceberg: |-
            connector.name=iceberg
            iceberg.catalog.type=hive_metastore
            hive.metastore.uri=thrift://hive-metastore:9083
            fs.hadoop.enabled=true

  python:
    helm:
      values:
        nodeSelector: *node_selector
        containers:
        - image: truevoid/python
          name: python
          command: [sleep, infinity]
        service:
          name: pyspark
          clusterIP: None
          ports:
          - containerPort: 10000
            port: 10000

dev:
  python:
    labelSelector:
      app.kubernetes.io/component: python
    sync:
    - path: python-hdfs:.
      excludePaths:
      - "*"
      - "!main.py"

  nessie:
    labelSelector:
      app.kubernetes.io/instance: nessie
      app.kubernetes.io/name: nessie

    ports:
    - port: "19120"

commands:
  initialzie: |-
    devspace enter -c python -- bash -c "HADOOP_USER_NAME=hadoop hdfs dfs -chown -R 777 /"

  trino: |-
    devspace enter -c python trino http://admin@trino:8080

  python: |-
    devspace enter -c python -- bash -c ". /root/.bashrc && poetry run python main.py"

  dump: |-
    kubectl exec postgres-postgresql-0 -- \
      bash -c "PGPASSWORD=hive pg_dump -Z9 -v -h localhost -U hive -d hive" \
      > $(date +'%s').tar.gz

  restore: |-
    helm uninstall --ignore-not-found hive-metastore

    kubectl exec postgres-postgresql-0 -- \
      bash -c "PGPASSWORD=hive dropdb -h localhost -U hive hive"

    kubectl exec postgres-postgresql-0 -- \
      bash -c "PGPASSWORD=hive createdb -h localhost -U hive hive"

    kubectl exec -i postgres-postgresql-0 -- \
      bash -c "PGPASSWORD=hive pg_restore --clean --if-exists -h localhost -d hive -U hive" \
      <$@

    devspace deploy --skip-build hive-metastore
